{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Exploratory%20Data%20Analysis/","title":"Exploratory Data Analysis","text":""},{"location":"Exploratory%20Data%20Analysis/#purpose","title":"Purpose","text":"<ul> <li>Used to identify patterns which can be observed by naked eye</li> </ul>"},{"location":"Exploratory%20Data%20Analysis/#types-of-analysis","title":"Types of analysis","text":"<ul> <li>Univariate - raw data info</li> <li>Bivariate - link between label and dependent variable</li> <li>Mulivariate - link between mltiple variables - genrally dimensionality reduction is used and visualised in 2D or 3D</li> </ul>"},{"location":"Exploratory%20Data%20Analysis/#examples-of-analysis-from-the-notebooks-in-the-site","title":"Examples of analysis from the notebooks in the site","text":"<ul> <li>Univariate - PIMA data (logistic regression): We plot each variable of the data and observe right skewed data. The outliers drag the regression line far so we clean up the data to reduce outliers.</li> <li>Bivariate - linear regression data: We plot relationship between income, age and experience and observe no clear reationship between age and income. So we go ahead and work with experience and income columns.</li> </ul>"},{"location":"Exploratory%20Data%20Analysis/#will-keep-on-updating-as-we-proceed","title":"Will keep on updating as we proceed","text":""},{"location":"linearRegression/","title":"Linear Regression","text":""},{"location":"linearRegression/#purpose","title":"Purpose","text":"<ul> <li>Helps find the relationship between features and label</li> <li>Generally 2 dim, but can be scaled to any dimension</li> <li>Any statistical model which can fit a linear equation can be approximately classified by linear regression</li> </ul>"},{"location":"linearRegression/#general-use-cases","title":"General use cases","text":"<ul> <li>Real estate: Predicting house prices based on square footage, number of bedrooms, and location. </li> <li>Marketing: Estimating sales based on advertising budget. </li> <li>Education: Analyzing the relationship between study hours and test scores. </li> <li>Healthcare: Predicting medical costs based on patient factors like age and health conditions. </li> <li>Agriculture: Estimating crop yield based on fertilizer and water levels</li> </ul>"},{"location":"linearRegression/#implementation","title":"Implementation","text":"<p><pre><code>import kagglehub\n\n# Download latest version from kaggle\npath = kagglehub.dataset_download(\"hussainnasirkhan/multiple-linear-regression-dataset\")\nprint(\"Path to dataset files:\", path)\n\n# Read csv\ndf = pd.read_csv(f\"{path}\" + \"/multiple_linear_regression_dataset.csv\", sep=\",\")\n\n#EDA\nimport matplotlib.pyplot as plt\nplt.subplot(2,1,1)\nplt.plot(df[\"experience\"], df[\"income\"], 'ro')\nplt.subplot(2,1,2)\nplt.plot(df[\"age\"], df[\"income\"], 'ro')\nplt.show()\n</code></pre> </p> <p>From the image we can observe that income is dependent on experience.</p> PythonsklearnPytorch <p><pre><code>import numpy as np\nclass linearRegressor:\n    def __init__(self) -&gt; None:\n        self.w = 0.3 \n        self.b = 0.4\n\n    def forward(self, X):\n        y_pred = [self.w*x + self.b for x in X]\n        return y_pred\n\n    def loss(self, y_pred, y_expected):\n        y_pred = np.array(y_pred)\n        y_expected = np.array(y_expected)\n        return np.mean((y_expected-y_pred)**2)\n\n    def backward(self, y, X, lr = 0.01):\n        f = y - (self.w*X + self.b)\n        N = len(X)\n        self.w -= lr * (-2 * X.dot(f).sum() / N)\n        self.b -= lr * (-2 * f.sum() / N)\n        return self.w, self.b\n\nlr = linearRegressor()\nlosses = []\ny_train  = df[\"income\"]\nX_train  = np.array(df[\"experience\"])\ny_pred = lr.forward(df[\"experience\"])\nlr.loss(y_pred, df[\"income\"])\nw,b = lr.backward(df[\"income\"], df[\"experience\"])\nfor i in range(200):\n    y_pred = lr.forward(X_train)\n    loss = lr.loss(y_pred, y_train)\n    lr.backward(y_train, X_train, lr=0.01)\n    losses.append(loss)\n# Plot the learning process\nplt.plot(range(len(losses)), losses)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.show()\n</code></pre> </p> <p><pre><code>plt.plot(df[\"experience\"], df[\"income\"], 'ro')\nplt.plot(X_train, y_pred, color = \"g\")\nplt.show()\n</code></pre> </p> <p><pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nmodel = LinearRegression()\nX_train = np.array(X_train).reshape(-1,1)\ny_train = np.array(y_train).reshape(-1,1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_train)\nprint(\"Coefficients: \\n\", model.coef_, model.intercept_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_train, y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_train, y_pred))\n\n# Plot outputs\nplt.scatter(X_train, y_train, color=\"black\")\nplt.plot(X_train, y_pred, color=\"blue\", linewidth=3)\nplt.show()\n</code></pre> </p> <p><pre><code>import torch\nimport torch.nn as nn\nclass linearRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(linearRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\nmodel = linearRegression(1, 1)\n\ncriterion = nn.MSELoss()\nlr = 0.005\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\nfor epoch in range(100):\n    inputs = torch.Tensor(X_train).requires_grad_()\n    labels = torch.Tensor(y_train)\n\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    print('epoch {}, loss {}'.format(epoch, loss.item()))\n\npredicted = model(torch.Tensor(X_train).requires_grad_()).data.numpy()\nplt.plot(X_train, y_train, 'go', label='True data', alpha=0.5)\nplt.plot(X_train, predicted, '--', label='Predictions', alpha=0.5)\nplt.legend(loc='best')\nplt.show()\n</code></pre> </p>"},{"location":"linearRegression/#when-does-linear-regression-fail","title":"When does linear regression fail?","text":"<ul> <li>Outliers: Extreme data points that can significantly skew the regression line. </li> <li>Non-linearity: When the relationship between variables isn't linear</li> <li>Collinearity: When independent variables are highly correlated with each other, making it difficult to isolate their individual effects. </li> <li>Heteroscedasticity: When the variance of the error terms is not constant across the data range.</li> </ul>"}]}